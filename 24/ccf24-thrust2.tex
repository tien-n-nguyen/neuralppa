\section{Thrust 1 (Task 2). Pre-training Language Models for Static Program Analysis}
%\label{sec:thrust2}

%\input{sections/ccf22-type}

As we delve into LLM-based multi-agent solutions, we also strive to create smaller models through pre-trained language models. Our approach is underpinned by the core idea that the knowledge gleaned from training with complete programs within expansive repositories of ultra-large-scale, open-source software can illuminate the analysis of partial code.

\input{ccf24-deeppda}
