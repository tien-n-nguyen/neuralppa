\section{Evaluation Plan}
\label{eval}

%Our {\em goals} of the evaluation plan include the studies to answer the questions:

%1) {\bf Intrinsic evaluation.} 

%2) {\bf Extrinsic evaluation.} How well do our proposed tools and methods help developers in improving the learning and usages of APIs in software libraries?

%3) How effectively do the proposed tools and methods help developers
%in real development processes?


\paragraph{\bf (1) Predictive Code Coverage Analysis.} We aim
to evaluate how {\tool} predicts code coverage for a given
test case without actual execution. We used two metrics: exact-match
accuracy and statement-match accuracy. The exact-match accuracy
quantifies the number of programs for which the predicted sequence of
statement/branch coverages precisely matches the target coverage
sequence, indicating perfect accuracy for all statements/branches
within the program. In contrast, the statement-match accuracy measures
the percentage of correctly predicted covered/not-covered
statements. While statement-match accuracy is aimed to evaluate
accuracy at the individual statement level, exact-match accuracy
evaluates at the entire code level. 

\paragraph{\bf (2) Efficiency of Test Case Generation}
Our goal is to further evaluate its efficiency by analyzing the number
of test cases needed to detect the next unique runtime error after the
previous detection in the evolutionary process of test case
generation. Specifically, we address the number of test cases it takes
for the coverage guided fuzzers to detect runtime exceptions that are
different from the ones previously detected. This study is conducted
primarily to understand the fuzzers' ability to move on to a different
local minima. The capacity to transition to another local minima
(i.e., detecting the next runtime errors/exceptions) with fewer test
cases underscores the fuzzerâ€™s ability to overcome the inefficiency
from the indiscriminate test case mutation in the con- ventional
coverage-guided fuzzing process.

\paragraph{\bf (3) Effectiveness of Generated Test Cases.}
We measure the effectiveness in detecting runtime errors. Within the
time limit, we counted the number of unique runtime errors/exceptions
that an approach can reveal for a program.  we also aim to measure how
the test generation processes from the approaches perform in terms of
the numbers of test cases created, wasted, executed, and the number of
effective test cases. Thus, we use the following metrics: Average
Generated Test cases (AGT), Average Executed Test cases (AXT), Average
and Effective Test cases (AET). AGT is computed as the average number
of generated test cases. AXT denotes the average
number of test cases that were actually executed on the programs to
detect runtime exceptions. AET denotes the average number of test
cases that effectively detected runtime errors.


\paragraph{\bf (4) Coverage Plateau.} The coverage plateau issue
hampers fuzzing effectiveness as it suggests the fuzzer might get
trapped in a local peak of code coverage, restricting its capacity to
uncover new bugs or vulnerabilities. The tool will be run on the
programs under test, and the test cases were recorded. Once the
session timed out for each fuzzer, unit tests were crafted for the
generated test cases, and the coverage was measured for each unit test
utilizing a Java Code Coverage tool.  For Jazzer, the test cases
generated at each interval of 0.1x of the complete generated test
suite were utilized to capture coverage.

\paragraph{\bf (5) Evaluation on proposed tools and methods in helping developers in real-world development.}

We will perform a set of user studies on the usefulness of our
proposed tools in the wild by releasing our
tools in the actual real-world development. 
%We will analyze all the aspects as in the previous studies and the total uptake of the tools.  
We will record feedback to improve our tools.
%PI Wang will connect with the Industry through our \textit{The New Jersey Innovation Institute} (NJII)~\cite{njii}, an NJIT corporation focusing on helping pri%vate enterprise.
UT-Dallas has a strong tie with companies like AT\&T, Facebook, and
Bloomberg. PI Nguyen will work with his existing collaborators in
Microsoft, IBM, Amazon research to perform user studies on the groups of
developers in certain tasks.
%to evaluate the usefulness of our proposed tools.
