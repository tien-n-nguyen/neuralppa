\subsection{Research Objectives and Anticipated Results}

%\begin{wrapfigure}{l}{0.62\textwidth}
%\begin{figure}[t]
%    \centering
%    \includegraphics[width=0.62\textwidth]{overview-new.png}
%    \vspace{-10pt}
%    \caption{Predictive Program Analysis: Analyzing Dynamic Program Behaviors}
%    \label{fig:arch}
%\end{wrapfigure}


We seek to advance the state-of-the-art in fuzz testing by means of
{\tool}, a {\bf Predictive Coverage-Guided Intelligent Fuzzing}
framework, with the goal of overcoming the issues listed in the
introduction. We aim to establish {\em a scientific foundation, novel
  methodologies, frameworks, models, and algorithmic solutions for
  predictive coverage-guided fuzz testing} with the following focus
areas:



(1) Large Language Models (LLM)-based {\bf predictive code coverage} (without test execution),

(2) LLM-based, targeting, {\bf test case generation}, and

(3) {\bf predictive coverage-guided fuzz testing} with LLMs.

%Our predictive program analysis framework is also beneficial in other
%scenarios in addition to programming assistants for incomplete code in
%an IDE. Predictive execution is not aimed to replace actual
%execution. Rather, it offers a solution where the actual execution is
%impossible, specifically in the scenarios 1) where the complete source
%code is not available, or 2) where the analysis or approximation on
%the behavior of the code is needed/desired without actual execution
%and some degree of inaccuracy in prediction is tolerable.  First, the
%first scenario can be exhibited in several examples, e.g., in the code
%snippets in Stack Overflow or GitHub gist. They often miss contextual
%information, such as imports and definitions of variables and
%functions. It can take a great deal of efforts to integrate such code
%snippets to a codebase. Second, the instances of the second scenario
%can include the analysis/approximation~of the behavior to check
%properties of untrusted code or~to~detect bugs early. Moreover, even
%with complete code, setting~up running environments for dynamic
%analysis is undesired due to missing third-party dependencies and
%complex build configurations.

%The \underline{key philosophy} that drives our work is that the analysis of
%partial code can be learned from the analysis of entire programs in
%the wealth of information obtained from ultra-large-scale, open-source
%software repositories.

To accomplish these tasks, we propose the following thrusts of
research in {\tool}:

\noindent \textbf{Thrust 1. LLM-based Predictive Code Coverage.} ({\em
  Section~\ref{sec:thrust1}})

In {\tool}, we aim to develop a LLM-based predictive code coverage
model to assess the quality of test cases, thereby selecting for
execution only those predicted to contribute to the higher total code
coverage of the test suite. Our framework involves a trade-off between
enhancing efficiency by avoiding the execution of every test case and
the potential risk of missing high-quality test cases due to
mispredictions on code coverage. We expect that with a sufficiently
high accuracy of code coverage estimation, the saving in execution
time for a significantly high number of low-quality test cases in
conventional coverage-guided fuzzing would outweigh any missed
high-quality test cases. We build our predictive code coverage model
based on our LLM-based preliminary approach in
CodePilot~\cite{forge24}. We can help it improve coverage prediction
via the incorporation of the actual execution results of the
collected test cases as few-shot learning examples.

We will develop a prompting approach based on program semantics that
collaborates with the LLM to predict code coverage. Specifically, to
tackle the intricacies of predicting code execution for coverage, we
leverage planning, which is pivotal, enabling an LLM to
autonomously generate plans based on exemplars. Our planning for code
coverage is grounded in program semantics, capturing the subtleties of
the execution of each statement. It integrates the synthesis of
Reasoning and Acting to predict the code coverage of a given code
snippet. The reasoning component encompasses the program-flow steps
that would have occurred had the code been executed. Each step in the
reasoning provides a concise explanation of whether a statement would
or wouldn't have been executed. This reasoning process guides the LLM,
directing its attention to detailed execution steps in its own plan
first by discerning various types of statements in a control
flow. Then, the LLM is requested to predict code coverage (Action)
based on the plan generated by itself (Reasoning).


%As depicted in Figure~\ref{fig:pre-rec}, conventional Program Analysis (PA) techniques exhibit high precision and recall when applied to complete code. However, when dealing with incomplete code, these techniques may experience a significant decrease in recall due to missing information, despite maintaining reasonably high precision or experiencing a slight decrease in precision due to their strict and/or heuristic analysis rules (denoted by PA $\medblacklozenge$ in Figure~\ref{fig:pre-rec}). On the other hand, employing an LLM directly for downstream analysis tasks may yield higher recall, thanks to the LLM's ability to explore the solution space (denoted by LLM $\circledast$ in Figure~\ref{fig:pre-rec}). However, the precision of   LLM analysis on incomplete code tends to be lower compared to conventional PA tools due to the lack of ability in specific analysis in the downstream tasks.

%We propose a tandem solution that combines LLMs and PA agents to
%leverage the strengths of both methodologies. We aim to harnessing the
%complementary capabilities of LLMs and PA: {\em LLMs' expansive search
%  capabilities} and {\em PA-based agents' semantic verification
%  abilities}.

%We advocate for a novel paradigm, called {\bf predictive program
%  analysis}, which operates on the principles of {\em
%  Approximation-Refinement} for Analysis. In the Approximation phase,
%a large language model (LLM) acts as a machine learning (ML) agent to
%fill in missing information within incomplete code. The missing
%information that will be filled by the LLM could be manifested {\em
%  explicitly}, e.g., in terms of missing variable declarations, setup
%API method calls, import statements, and exception handling types, or
%{\em implicitly}, e.g., in terms of missing type information of 
%program elements, missing dependencies, etc.

%\input{precision-recall-plot}

%A naive solution is to take whatever information the LLMs completed
%and feed to a traditional program analysis as is. However,
%researchers have shown that while LLMs are remarkable in code
%generation with correct syntaxes, the produced code often contains
%semantic errors and even do not compilable.  Thus, the Refinement
%phase employs a program analysis (PA)-based agent to verify the
%completed code output by the LLM. This PA-based agent utilizes the
%compiler technology to ensure that the generated code is compilable
%and consistent with used external libraries. The iterative interplay
%between the LLM-based agent and the PA-based agent continues until a
%compilable code is achieved.
%In brief, the interplay between LLM and PA agents has two objectives:
%1) to enhance recall compared to the PA-only solution (via the
%Approximation of the incomplete code), and 2) to
%maintain or even improve precision beyond what can be achieved with
%LLM-only solution by integrating PA to ``correct'' the LLM's solution
%(via the result Refinement).

%While exploring LLM-based multi-agent solution, we also aim to develop
%smaller models via pre-trained language models. Our work is driven by
%the fundamental belief that insights gained from training by complete
%programs within vast repositories of ultra-large-scale, open-source
%software can inform the analysis of partial code~\cite{naturalness-icse12}.
%Thus, we expect to build/fine-tune pre-trained LLM models to learn
%from those repositories.

\vspace{3pt}
\noindent \textbf{Thrust 2. A Tandem of LLMs for Test Case Generation and Code Coverage Prediction}  ({\em Section~\ref{sec:thrust2}})

In framework, we utilize a tandem of LLMs, each serving distinct
roles: one for test generation from the source code and the other for
code coverage prediction (CodePilot). The first LLM, which replaces
the conventional mutation engine, collaborates with the second LLM to
iteratively produce higher quality test cases concerning code
coverage, thereby mitigating the impact of coverage plateaus and
enhancing coverage of runtime exceptions based on the knowledge
provided by the LLMs about the given code.

The primary objective of the first LLM is to expand the exploration of
the test case space for the given source code, thereby increasing the
recall. However, this may come at the expense of precision in the
generated test cases. To counterbalance this trade-off, the second LLM
in the code coverage prediction module, CoPilot~\cite{forge24}, aims to
enhance precision by assessing the quality of the generated test cases
in terms of code coverage. Furthermore, any duplicate test cases
generated by the first LLM are discarded to maintain efficiency and
avoid redundancy in the test suite.



%We advocate for an execution paradigm called predictive execution. In
%predictive execution, with a specific input, the execution is not
%carried out with the computer performing the instruction in the
%program. Instead, a trained machine learning model predicts the
%execution steps and as a result, the execution trace corresponding to
%the input is derived without actual execution.

%By simulating program behaviors and outcomes without running the code,
%these approaches provide valuable insights into the program's
%potential runtime characteristics. Here are some benefits of
%predictive execution. First, early error detection: predicting program
%executions allows for the early detection of errors and potential
%vulnerabilities before executing the code. By simulating different
%execution paths and scenarios, predictive execution can enable the
%dynamic analysis tools to identify potential issues such as null
%pointer dereferences, memory leaks, or buffer overflows without the
%need to execute the code in a real-world environment. This early error
%detection helps developers catch and fix bugs more efficiently,
%reducing the likelihood of critical issues in production. Second,
%dynamic analysis techniques that predict program executions play a
%crucial role in security analysis. By simulating potential attack
%scenarios and analyzing how the program behaves under different
%conditions, these tools can identify security vulnerabilities such as
%injection attacks, privilege escalation, or data breaches. Predictive
%execution helps security professionals assess the resilience
%of systems against various threats and vulnerabilities,
%enabling proactive security measures and robust defenses. Finally,
%predicting executions provides developers with a deeper understanding
%of their code's behavior without the need for actual execution. By
%simulating program flows and interactions, tools can uncover hidden
%dependencies, identify unexpected behaviors, and reveal complex
%program dynamics.

%In addition multi-agent solutions, we also explore the
%pre-training language models. We aim to teach a smaller model to analyze
%the runtime behaviors via the learning from complete programs in execution.

\vspace{3pt}
\noindent \textbf{Thrust 3. Predictive Coverage-Guided Intelligent Fuzzing}  ({\em Section~\ref{sec:thrust3}})

{\em We design the integration/interleaving of two loops} to serve a
dual purpose. Firstly, the first loop (1) depicted in
Figure~\ref{fig:fuzzwise} directs the test-generating LLM to produce
related test cases that address a specific faulty area within the
source code. This function parallels that of a mutation mechanism,
generating test cases centered around a seed. In contrast, the
second loop (2) is tailored to guide the LLM in moving on to a
different faulty area, i.e., generating test cases that cover other
untested areas. Moreover, the two loops are structured to address
two primary objectives. The first loop (1) steers the LLM towards
generating test cases that contribute to overall code coverage, while
the second loop (2) prioritizes the detection of bugs and runtime
errors. In general, the incorporation of the tandem of LLMs and the
integration of feedback loops for both aim to enhance efficiency
and effectiveness. By leveraging the
complementary roles of the LLMs, we strive to generate
fewer but higher-quality test cases, while also improving its ability
to detect defects.

%Our last thrust of research is aimed to demonstrate the usefulness of
%our solution in different programming assistant applications: 1)
%vulnerability detection, 2) dynamic slicing,  etc.

\input{ccf24-fuzz-plan-table}
