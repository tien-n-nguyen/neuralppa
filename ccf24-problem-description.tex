%\section{Introduction}

Program analysis is the process of examining a program to reason about
its properties, structures, or behaviors. There are two broad
categories of program analysis: {\em static} and {\em dynamic}. The
former uses program semantics to examine all possible behaviors that
might arise when the code is executed, while the latter uses actual
execution for specific inputs to analyze and establish run-time
behaviors.

Static program analysis involves examining the source code or compiled
code without executing it. This category of approaches aims to
identify and establish program behaviors by analyzing the program
semantics including code's structure, control and data flows, program
dependencies, etc. Static analysis approaches can provide developers
with valuable insights into their codebase before execution.
On the other hand, dynamic program analysis involves analyzing the
behavior of a program during its execution. This approach requires
running the program with various inputs to observe its runtime
behavior, e.g., memory usage, variable values, and function
calls. By monitoring program execution, dynamic analysis can uncover
runtime errors, memory leaks, and performance bottlenecks that may not
be apparent through static analysis alone. Dynamic analysis provides
real-time feedback on how the program behaves under different
conditions and inputs, making it particularly useful for uncovering
runtime issues in complex or large-scale applications.

While achieving successes, both categories of program analysis
approaches have key limitations. Static analysis tools often adopt
conservative approaches to ensure thorough coverage (completeness),
leading to the detection of behaviors that may never manifest in
real-world execution scenarios. Static analysis tools may produce
false positives (reporting behaviors that are not actual) or false
negatives (missing actual results), sometimes requiring manual
verification to differentiate between real behaviors and false
alarms. Unlike the common overestimation of static analysis, dynamic
analysis may not cover all possible execution paths or scenarios,
leading to underestimation of program behaviors that occur only under
specific conditions or inputs. However, when dynamic analysis reports
certain properties, they tend to be actual program behaviors
(soundness). Therefore, dynamic analysis often achieves more sound
(higher precision) than static analysis, while the latter often
achieves more complete (higher recall) than the former.

%Tien

It is desirable to have an approach that can strike a balance and
obtain the best of both worlds. To this effect, we propose a novel
paradigm called {\bf predictive program analysis}, which aims to learn
to analyze program behaviors from the semantic and execution behaviors
of programs obtained from ultra-large-scale, open-source software
repositories.

%The fundamental hypothesis toward achieving such an approach is
%whether one can leverage the knowledge of behaviors through the
%execution of the sufficient number of test cases and generalize it for
%the given source code.

%------------
You might want to mention that the hypothesis of predictive program analysis is that the naturalness of software, i.e., program properties are predictable, etc.

You should add a paragraph to discuss about the principle of predictive PA, which leverages the knowledge on program behaviors from dynamic analysis and generalizes it for all program inputs.

If the dynamic information is not available due to the incompleteness of source code, predictive program analysis will fill in the missing information and then produces the training data by executing the complete code.

With the intelligent generalizability of advanced machine learning, we expect that the predictive pa will be more complete than dynamic analysis, yet more sound than static analysis.
%---------------


...since it is not dependent on the completeness of the program, we can extend such analyses to code
fragments from developer forums such as StackOverflow, which
further helps assess and enhance their usability. -----> you might want to move this to the dynamic analysis paragraph (Second,...) because some static analyses do not require completeness of source code.

Predictive PA helps reduce the false positive and overestimation by static analysis because it has the runtime knowledge and generalization capability of ML models.

In the paragraph for dynamic analysis, you might want to touch 1) the incomplete code snippets and 2) the potential improvement in terms of recall (completeness). For 1), you had it and you can add the texts on stackoverflow from the static analysis paragraph here. For 2), you can explain why a ML model can learn from the dynamic info for some inputs and generalize it for unseen inputs.

In Section 2.1, you might want to touch on how we train the model by executing the complete code.

In 2.2, you might want to emphasize on the fact that we actually do not run the code. The section sounds like we run the code.

Section 3.1, you might want to remind the readers that the above approaches that we explained so far except for NeuralPDA built the training data by actual execution. If the code is incomplete, we will use the framework described here in 3.1.


{\bf AY} Static analysis tools are often designed to guarantee that its results
provide a complete description of the program properties, while
conservatively skipping the properties about which it cannot provide a
guarantee. This is not always useful. Moreover, a perfectly precise
static analysis that considers every possible execution of a program
is not feasible either. For instance, it is not possible to enumerate
all execution paths in programs containing infinite loops, or when
missing necessary dependencies.

Dynamic analysis, in contrast, provides precise observations over
executions of the test suite. By design, it is an under-approximation
and can not generalize, as the test suite might not be representative
of all possible execution paths the program can take.



{\textcolor{red}{OLD MATERIALS}}
Integrating the utilization of program analysis (PA) tools early in the development process of modern, large-scale software systems is crucial in determining the weaknesses/vulnerabilities in the code. Consider, for instance, a scenario in which a developer wants to use an online question and answering (Q\&A) forum, e.g., StackOverflow (S/O), to learn how to use software libraries or frameworks. Typically, the answer to a posed question comes as a fragment/chunk of code, which later makes it to the production application, stemming from the copy-and-paste software reuse practice. Unfortunately, if the copied code fragment is vulnerable, i.e., possesses defects that one can potentially exploit, it will result in the application being prone to attacks. Verdi {\em et al.}~\cite{verdi-tse22} reviewed more than 72K C++ code snippets that migrated from 1,325 S/O answers, reporting a total of 99 vulnerable code snippets of 31 different types that made their way to 2,589 GitHub repositories.

Security researchers have proposed several automated approaches for vulnerability detection (VD) in software systems using program analysis (PA)~\cite{FlawFinder,RATS,viega2000its4,Checkmarx,HPFortify,Coverity}, as well as machine learning and deep learning (ML \& DL) \cite{fse21,chakraborty2020deep,zhou2019devign,li2018sysevr,li2018vuldeepecker} techniques. These approaches typically leverage program representations such as abstract syntax tree (AST), program dependence graph (PDG)~\cite{fse21,li2018vuldeepecker}, control-flow graph (CFG)~\cite{zhou2019devign}, data-flow graph (DFG)~\cite{zhou2019devign}, code property graph (CPG)~\cite{chakraborty2020deep}, etc., to model the vulnerable features. 
%However, extending such analyses to code snippets is not straightforward as they are often incomplete, unparseable, contain declaration/reference ambiguity, and may be interspersed between user comments. Currently, there exist tools such as PPA~\cite{ppa08}, which parse an incomplete code fragment to build the AST and extract data types in a best-effort manner, while StaType \cite{icse18} resolves the libraries and recovers only the fully-qualified names for references. However, the basic infrastructure for partial program analysis, i.e., for analyzing incomplete code is not yet available. 
%Such an infrastructure must include fundamental supports/services at the structural, semantic, and execution levels, thus enabling the static and dynamic analysis techniques to be built upon. 
%Let us refer to this as \textit{partial program analysis infrastructure}.

While deep learning (DL) and advanced machine learning (ML) with large
language models (LLMs) have been achieving remarkable successes in the
area of ML for code, the application of ML/DL and LLMs in
vulnerability detection for incomplete code snippets is still
limited. The program analysis (PA) tools employed to derive these
program representations warrant the code to exist as complete program
units, at the very least, at the method-level granularity. As a
result, it is impossible to utilize such VD tools to find
vulnerabilities in code snippets. A possible alternative would be to
plug the code snippet into the method, resolve any ambiguities, and
test it with a VD tool. However, such a strategy is limited. First, if
a vulnerability is found, the efforts of integrating the code snippet
into the existing method would be lost. Second, due to the black-box
nature of DL models, we would not know the origin of the
vulnerability, i.e., whether it arises due to the flawed code snippet
or the existing part of the code.

Besides, analyzing code snippets is not straightforward as they are often incomplete, un-parseable, contain declaration/reference ambiguity, and may be interspersed between user comments. Currently, there only exist tools such as PPA~\cite{ppa08}, which can parse an incomplete code fragment to build its AST and extract the declared data types in a best-effort manner; and StatType \cite{icse18}, which can resolve the libraries and recover the fully-qualified names for API references. However, the basic infrastructure for partial program analysis, i.e., for analyzing incomplete code is not yet available. Such an infrastructure must include fundamental supports/services at the structural and semantic levels, enabling traditional program analysis techniques to be extended to incomplete code. We refer to this as \textit{partial program analysis infrastructure}.


Currently, performing program analysis for a specific code snippet
necessitates the access to the entire program containing the
snippet. The need for partial program analysis arises in situations
where only partial code is accessible, such as in a vulnerable code
snippet in an online forum, a commit log or code diff, or when partial
code is sent to a server due to security-related concerns. Without
partial program analysis, the tasks including {\em early vulnerability
detection or assessment would be very limited and even impossible}.

%In addition to vulnerability detection, such an infrastructure is also
%beneficial to other software engineering (SE) tasks that can tolerate
%a low level of errors and imprecision in building program
%representations. For example, consider code
%completion~\cite{codefill-icse22,facebook-icse21}, in which a model
%provides suggestions to complete partial code. Existing
%state-of-the-art ML/DL-based code completion models are just based on
%the code sequences or utilize the syntactic structure in ASTs, but
%none leverages the program dependencies due to the nature of partial
%code. Next, consider the task of analyzing the code fragments in a bug
%report to connect it to the relevant source files for bug localization
%purposes~\cite{euler-fse19,icpc17}. Here too, a need for partial
%program analysis, specifically partial program dependence analysis,
%can be observed.

%The other facets of program analysis tools used in vulnerability
%detection that need attention are their soundness and
%completeness. For example, static analysis tools are designed to
%detect errors that are valid for all possible executions, which often
%come at the cost of multiple approximations. As a result, such tools
%tend to overestimate, resulting in false positives. Next, consider the
%task of symbolic execution, which is limited by the path explosion
%problem that hinders its applicability to large-scale software
%systems. The effectiveness of such a symbolic execution engine can be
%improved by guiding it to explore the right subset of symbolic states.

