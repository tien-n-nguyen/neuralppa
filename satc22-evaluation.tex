\section{Evaluation Plan}
\label{eval}

We will conduct empirical studies to evaluate {\tool} with the
following questions: 1) How well does {\tool} perform in
helping users detect undiscovered vulnerabilities? 2) How well does
{\tool} perform in helping users perform forensic investigation when
security breach occurs? 3) How well does {\tool} help in the
mitigation process? We will conduct two kinds of evaluation: intrinsic
(via internal evaluation metrics) and extrinsic (via controlled
experiments with human studies). We will use datasets of known
vulnerabilities, such as software vulnerability datasets (e.g., Fan
{\em et al.}'s~\cite{fan2020msr}, Reveal~\cite{chakraborty2020deep}
and FFMPeg+Qemu~\cite{zhou2019devign}).  Fan {\em et
  al.}~\cite{fan2020msr} dataset covers CVEs from 2002 to 2019 with 21
features for each vulnerability.  At the method level, the dataset
contains +10K vulnerable methods and fixed code. The Reveal
dataset~\cite{chakraborty2020deep} contains +18K methods with 9.16\%
of the vulnerable ones. The FFMPeg+Qemu dataset has been used in
Devign study~\cite{zhou2019devign} with +22K data, and 45.0\% of the
entries are vulnerable.
%We will use the following metrics to evaluate the vulnerability
%detection results and the forensic investigation results in the
%experiments for the research questions 1) and 2): precision (the
%number of correctly detected vulnerabilities over the total number of
%detected ones), recall (the number of correctly detected
%vulnerabilities over the total vulnerabilities). Since
Because the tools will
provide the ranked list of potentially vulnerable code, we will also
use the following ranking metrics: {\bf {\em Mean Average Precision}}
$MAP$ $=\frac{\sum_{q=1}^{Q}AvgP(q)}{Q}$, with {\em Average Precision}
$AvgP$ $=\sum_{k=1}^{n}P(k)rel(k)$,
%
where $n$ is the total number of results $k$ is the current rank in
the list, $rel(k)$ is an indicator function equaling to 1 if the item
at rank $k$ is actually vulnerable, and to zero otherwise. $Q$ is the
total number of classification types. It is $1$ because we only have
two types including vulnerable and non-vulnerable classes, however, we
rank all the methods based on their scores (1 indicates vulnerable,
and 0 otherwise).
%
{\bf {\em Normalized DCG}} at $k$: {\bf $nDCG_k$}
$=\frac{DCG_k}{IDCG_k}$, with {\em Discounted Cumulative Gain} at rank
$k$, $DCG_k$ $=\sum_{i=1}^{k}\frac{r_i}{log_2(i+1)}$; and {\em Ideal
DCG} at $k$ $IDCG_k$
$=\sum_{i=1}^{|R_k|}\frac{2^{r_i}-1}{log_2(i+1)}$; where $r_i$ is the
score of the result at position $i$, and $R_k$ the rank of the actual
vulnerable methods (ordered by their scores) in the resulting list up
to the position $k$.
%
{\bf First Ranking} ($FR$) is the rank of the first correctly
predicted vulnerable method. {\bf Average ranking} ($AR$) is the
average rank of the correctly predicted vulnerable
methods in the top-ranked list.
%
{\bf Accuracy under curve} (AUC) is defined as $AUC = P(d(m_1)>d(m_2))$ in
which $P$ is the probability, $d$ is the detection model (can be
regarded as a binary classifier), $m_1$ is a randomly chosen positive
instance, and $m_2$ is a randomly chosen negative one.

For all three research questions, we will further conduct the controlled experiments with human subjects to evaluate how well {\tool} helps users in vulnerability detection, investigation, and mitigation. Human subjects will be divided into two groups. One group will be equipped with XAI-VDA tool supports, and another group will not. We will measure the quality of the detection or investigation via~precision, recall, and ranking metrics, and the time to finish the task of vulnerability detection or investigation.

%We will also create several hypothetical incidents and forensic investigations, with two groups (one with XAI-VDA and the other without). We will measure the final findings and the time efficiency in retrieving the forensic evidence. A key aspect of this evaluation is the ability to support the forensic investigators in accurate and efficient evidence collection that helps reveal the vulnerable devices and/or software vulnerabilities. 

%Our {\em goals} of the evaluation plan include the studies to answer the questions:

%1) {\bf Intrinsic evaluation.} 

%2) {\bf Extrinsic evaluation.} How well do our proposed tools and methods help developers in improving the learning and usages of APIs in software libraries?

%3) How effectively do the proposed tools and methods help developers
%in real development processes?


\paragraph{\bf Evaluation on proposed tools and methods in helping developers in real-world development.}

We will perform a set of user studies on the usefulness of our
proposed tools in the wild by releasing our
tools in the actual real-world development. 
%We will analyze all the aspects as in the previous studies and the total uptake of the tools.  
We will record feedback to improve our tools.
%PI Wang will connect with the Industry through our \textit{The New Jersey Innovation Institute} (NJII)~\cite{njii}, an NJIT corporation focusing on helping pri%vate enterprise.
UT-Dallas has a strong tie with companies like AT\&T, Facebook, and
Bloomberg. PI Nguyen will work with his existing collaborators in
Microsoft, IBM, ABB research to perform user studies on the groups of
developers in certain tasks to evaluate the usefulness of our proposed
tools.
