\section{Thrust 1: Proactive Software Vulnerability Detection with Explainable AI}
\label{sec:thrust1}

\subsection{Vulnerable Source Code Detection with Graph-based Convolution Network} \label{sec:vd}

\begin{figure}[!hbt]
    \centering
    \includegraphics[width=6.5in]{graph-based-CNN.png}
    \caption{Feature-Attention, Graph-based Convolution Network for
      Vulnerability Detection Model}
    \label{fig:detection-model}
\end{figure}


\noindent \underline{\bf Abstractions and Representations}. A vulnerability is usually
exhibited as multiple statements are exploited, thus, it is natural to
capture the vulnerable code as a sub-graph in the PDG with the data
and control flows. To do so, we model the vulnerability
detection via the Graph Convolutional Network (GCN)~\cite{GCN16} as
follows. The PDG of a method $M$ is represented as a graph $G_N$ =
$(V,E)$ in which $V$ is a set of nodes representing the statements,
and $E$ is a set of edges representing the data/control dependencies.
%
A feature description $x_V$ is for every node $v$, which represents a
property of a node, e.g., variable name, etc. Features are summarized
in a $N \times D$ feature matrix $X_M$ ($N$: number of nodes and $D$
is the number of input features).
%
Let $f$ be a label function on the statements and methods $f: V
\rightarrow \{1,...,C\}$ that maps a node in $V$ and an entire method
to one of the $C$ classes. We could have vulnerable
($\mathcal{V}$) and non-vulnerable ($\mathcal{NV}$).


To train on (non-)vulnerable code, GCN
performs similar operations as CNN where it learns the features with a
small window sliding over PDG sub-structure. Differing from
image data with CNN, the neighbors of a node in GCN are unordered and
variable in size. To predict if a method $M$ is vulnerable, its PDG
$G_M$ with the associated feature set $X_M$ = $\{x_j|v_j \in G_M\}$
are built. GCN learns a conditional distribution $P(Y|G_M,X_M)$, where
$Y$ is a random variable representing the labels $\{1,...,C\}$. That
distribution indicates the probability of $G_M$ belonging to
each of the classes $\{1,...,C\}$, i.e., $M$ is vulnerable or not.

Figure~\ref{fig:detection-model} shows our design of the Graph-based
Convolution Network (GCN) model for {\em proactive vulnerability
  detection at the source code level}. We also plan to use
Feature-Attention to enhance the GCN, and hereafter we will refer to
the model as FA-GCN. First, the source code under investigation will
be parsed and the program dependence graph (PDG) is extracted.  For
example, in Figure~\ref{fig:detection-model}, D1-D9 represent the
statements in a program under investigation. The edges represent the
control and data dependencies among the statements. For each
statement, we can use any set of features/attributes that are
important to the statements.
%Each node can be associated with attributes.

%devices in an IoT system are modeled as the nodes in a graph structure
%in a GCN. D1-D9 in Figure~\ref{fig:detection-model} are nine example
%IoT devices under study in the system, where the attributes and
%features of a node represent the properties associated with the IoT
%devices.

Similar to CNN using the filter on an image, FA-GCN performs sliding a
small window along all nodes (statements) of the PDG. For example, in
Figure~\ref{fig:detection-model}, the window marked with \circled{A}
for the node $D1$ consists of itself and the neighboring
statements/nodes $D2$, $D4$, $D5$, and $D8$. Another window (marked
with \circled{B}) is for the node $D7$ at the center, including itself
and the neighboring nodes: $D5$ and $D8$. For each window, FA-GCN
generates the feature representation matrix for the statement at the
center. For example, for the window centered at $D1$, it generates the
feature vector $F_{D1}$ for the statement $D1$. 

From the representation vectors for all statements, FA-GCN uses a join
layer to link all these vectors into the Feature Matrix
$\mathcal{F}_{m}$ for method $M$. A row in $\mathcal{F}_m$ corresponds
to a window in the input PDG graph.  Next, FA-GCN performs the
convolution operation by first calculating the symmetric normalized
Laplacian matrix~$\tilde{A}$~\cite{GCN16}, and then calculating the
convolution to generate the representation matrix $M_{m}$ for the
subset $S$ of the source code under investigation. After that, we use
the traditional steps as in a CNN model: using a spatial pyramid
pooling layer (to normalize the method representation matrix into a
uniform size, and reduce its total size), and connecting its output to
a fully connected layer to transform the matrix into a vector $V_S$ to
represent the subset $S$ of the IoT devices under study. With $V_S$,
we perform classification by using two hidden layers (controlling the
length of vectors and output) and a softmax function to produce a
prediction score for $S$. We use those scores as {\em vulnerability
  scores to rank the potentially vulnerable code}. The decision for
$m$ as vulnerable or not can be done via a trainable threshold on the
prediction score~\cite{li2018vuldeepecker,li2019improving}. The
detection results in terms of the scores for all statements are
recorded.

%Let us revisit {\em example scenario 1} presented in Section \ref{section:An Example Use Case}, where we assume that IoT devices and computation within them, particularly those that are publicly located within the campus network, can be compromised and used to facilitate other malicious cyber activities. 
% To model this scenario, each node in the FA-GCN model represents an IoT device (e.g., nodes for servers, smartTV, networks, camera, etc.), and the edges represent the interconnections among the devices. To train the FA-GCN, we can use the historical data from the same or similar IoT systems with their devices in the same device category. The data acquisition process is explained in Section~\ref{subsection:Data Acquisition}.

For training, the Vulnerability databases, e.g., Common
Vulnerabilities and Exposures in the National Vulnerability Database
(NVD) can also be used for training the vulnerability detection model.

%\noindent {\bf Risk Scanning Component}. A similar architecture can be used for the risk scanning component in an IoT system. In this case, the input graph is used to represent the connection structures among IoT devices. The attributes of the nodes represent the features and the {\em risk scores for the IoT devices}. To train the model to evaluate the risk scores, we can rely on the historical data of the devices over time. 

\subsection{Explainable AI for Vulnerability Detection}
\label{sec:xai}

%\begin{figure}[!hbt]
%    \centering
%    \includegraphics[width=3.8in]{xai-example.png}
%    \caption{Graph-based Explainable Model for Vulnerability Detection}
%    \label{fig:xai}
%\end{figure}

\begin{wrapfigure}{l}{0.55\textwidth}
	\centering
	\includegraphics[width=3.6in]{xai-example.png}
	\caption{Graph-based Explainable Model for Vulnerability Detection}
	\label{fig:xai}
\end{wrapfigure}

Let us explain our proposed Explainable AI model.  The explainable AI model takes the detection result with
the vulnerability scores for all the statements and the FA-GCN
detection model itself to produce the {\bf Explainable Set}, {\em
  which is defined as the set containing the parts of the input
  explaining the reason for the model to decide the detected
  vulnerability}. Specifically, it uses both the input graph $G_M$ of
the PDG and the FA-GCN model as the input to obtain the Explainable
Set.  To that end, our goal is to take the FA-GCN and a specific input
graph $G_M$ for PDG of the code under study, and produce the {\em
  crucial sub-graph structures} and {\em crucial features} in $G_M$
that affect the decision of the FA-GCN detection model.

We leverage our preliminary work on XAI~\cite{fse21-submission}: our
idea is that {\em if removing or altering a node or a feature in the
  input PDG graph of the code does affect much the prediction scores,
  the node or the feature is considered as essential and thus must be
  included in the explainable set}. We use
GNNEXplainer~\cite{GNNExplainer} as follows. It searches for a
sub-graph $\mathcal{G}_M$ in $G_M$ that minimizes the difference in
the prediction scores between using the whole graph $G_M$ and the
minimal graph $\mathcal{G}_M$. Without that subgraph $\mathcal{G}_M$
in the input $G_M$, the model would not decide $G_M$ as vulnerable,
thus, $\mathcal{G}_M$ is considered as {\bf crucial PDG sub-graph}
consisting of {\bf crucial features} in those statements and
connections {\bf relevant to the detected vulnerability}.

\noindent {\em Potential Impacts of Explanations}.  The
statements with crucial features in both Explainable Sets will be
visualized and presented to the security expert. Based on his/her
expertise, (s)he can quickly omit the benign code from the list of
potentially vulnerable code.

%Correctly detected vulnerable device(s) will be isolated and prevented
%from further interaction in the network (i.e., no sending or receiving
%data). The input graph representing the adjusted IoT system with the
%new graph structure of IoT devices will be the input for the next
%iteration of detection.

%Let us revisit {\em example scenario 1} in Section~\ref{section:An Example Use Case} again. Assume that after the first iteration, the model detects that nodes corresponding to servers A, B, and C, router D,  camera E, and  smart TV F are vulnerable. However, with the expertise and knowledge from the expert, router D, which cannot be accessed from the outside, can be quickly eliminated from the suspicious list. The model in the next iteration will not consider router D in the detection process and its parameters will be updated accordingly. The process will continue with the integration of human experts in the loop.
   

\noindent \underline{\bf Formulation.}
%Let us formulate how we build the explainable sets and
%features/attributes. The input includes the trained FA-GCN model, the
%input ($G_M$), and the detection scores for all the
%nodes. Figure~\ref{fig:xai} illustrates our algorithm to train the
%model.
To derive the explainable sets, the key goal is to find a sub-graph
$\mathcal{G}_M$ in the input $G_M$ that minimizes the
difference in the prediction scores between using the entire graph
$G_M$ and using the minimal graph $\mathcal{G}_M$. To do so, we use
GNNExplainer's {\em masking technique}~\cite{GNNExplainer},
which treats the searching for the minimal graph $\mathcal{G}_M$ as a
learning problem of the {\em edge-mask} set $EM$ of the edges,
and the {\em feature-mask} set $XM$ of the features.
The idea is that learning these two sets $EM$ and $XM$
helps {\tool} derive the explainable sub-graph $\mathcal{G}_M$
and the set of crucial feature $\mathcal{X}_M$
by masking-out the edges in $EM$
and the feature in~$XM$ from $G_M$ (input graph) and $F_M$ (input feature)
({\em Note}: ``masked-out'' is denoted by $\bigodot$):
\begin{equation}\label{eq:11}
\mathcal{G}_M = G_M \bigodot EM
\end{equation}
\begin{equation}
\mathcal{F}_M = F_M \bigodot XM
\end{equation}
In Figure~\ref{fig:xai}, the explainable module checks if the FA-GCN model produces the same result (in this case the result is
vulnerable). If yes, the edge in the edge-mask is not important and
is not included in $\mathcal{G}_M$. Otherwise, the edge is important
and included in $\mathcal{G}_M$.
Similar treatment is for {\em feature-mask} $XM$. In
Figure~\ref{fig:xai}, $FM$ masks the second and fourth features for
each node (IoT device) (Each device has the same number of
features).
Because the numbers of possible sub-graphs and the edge-mask sets
and feature-mask
are untractable, GNNExplainer uses a learning approach for the
edge-mask $EM$ and feature mask $XM$.

\input{formulation}





%====================================

\subsection{Illustrated Example for XAI}



Let us illustrate the usage of our framework on a real-world vulnerable code. Figure~\ref{fig:pdg} shows the
method \code{ec$\_$device$\_$ioctl$\_$xcmd} in Linux 4.6, which
constructs the I/O control command for the CromeOS devices. This is a
known vulnerabiluty (i.e., CVE-2016-6156). The commit log of the
corresponding fix stated that {\em ``At line 6 and line 13, the driver
  fetches user space data by pointer \code{arg} via
  \code{copy$\_$from$\_$user()}''}.


\begin{wrapfigure}{l}{0.55\textwidth}
	\centering
	\includegraphics[width=3.6in]{pdg-2.png}
	\caption{Explanation in terms of PDG subgraph}
	\label{fig:pdg}
\end{wrapfigure}

The first fetched value (stored in \code{u$\_$cmd}) (line 6) is used
to get the \code{in$\_$size} and \code{out$\_$size} elements and
allocation a buffer (\code{s$\_$cmd}) at line 10 so as to copy the
whole message to driver later~at~line 13, which means the copy size of
the whole message (\code{s$\_$cmd}) is based on the old value
(\code{u$\_$cmd.outsize}) from the first fetch. Besides, the whole
message copied at the second fetch also contains the elements of
\code{in$\_$size} and \code{out$\_$size}, which are the new
values. The new values from the second fetch might be changed by
another user thread under race~condition, which will result in a
double-fetch bug when the inconsistent values are used.
%
Thus, to fix this bug, a developer added the code at lines 17--21 to make sure that \code{u$\_$cmd.outsize} and \code{u$\_$cmd.insize} have not changed due to race condition between the two fetching calls. Moreover, memory access might be also beyond the array boundary, causing a buffer overflow within the method call \code{cros$\_$ec$\_$cmd$\_$xfer(...)}, when the command is transferred to the ChromeOS device at \underline{line 23}.

Another issue is with
\code{copy$\_$to$\_$user}, \underline{line 27}. The method call \code{cros$\_$ec$\_$cmd}
\code{$\_$xfer(...)} can set \code{s$\_$cmd-$>$insize} to a lower
value. Thus, the new smaller value must be used to avoid copying too
much data~to the user: \code{u$\_$cmd.insize} at line 27 is changed
into \code{s$\_$cmd-$>$insize}.

%This vulnerable code could potentially cause damages such as denial of
%service, buffer overflow, and program crash. Recent advances in Deep
%learning (DL) advances enable several
%approaches~\cite{zhou2019devign,li2018vuldeepecker} to {\em implicitly
%  learn} from the history the patterns of vulnerable code, and to
%detect {\em more general vulnerabilities}.  However, they are limited to
%provide a binary answer on yes or no w.r.t. vulnerability.
%they are still limited in comparison with program analysis-based
%approaches in their ability to provide details on the {\em
%  fine-grained} level of the vulnerable statements, and on why the
%model has decided on the vulnerability.  For example, PA-based
%approaches, e.g., a race detection technique could potentially detect
%the involvement of the two fetching statements at lines 6 and 13. The
%method in Figure~\ref{fig:motiv_1} might be deemed as vulnerable by a
%DL-based model. However, without any fine-grained details, a developer
%would not know where and what to investigate next. This would make the
%output of a DL model less constructive in VD. Moreover, a fault
%localization technique~\cite{keller2017critical}, which locates buggy
%statements, would need a large, effective test suite.

%For detection, existing DL-based
%approaches~\cite{zhou2019devign,li2018vuldeepecker} do not fully
%exploit all the available information on the vulnerable code during
%training. For example, during training, we know that lines 23 and 27
%are vulnerable/buggy, and other {\em relevant statements via
%  data/control dependencies provide contextual information for the
%  vulnerable ones}. However, the existing
%approaches~\cite{zhou2019devign,li2018vuldeepecker} do not consider
%the vulnerable statements and do not use the contextual code to help a
%model discriminate the vulnerable and non-vulnerable ones. The entire
%method would be fed to a DL model.

We propose an DL-based, {\em interpretable vulnerability detection}
approach that goes beyond the decision of vulnerability by providing
the interpretation in term of the vulnerable
statements. Specifically, as the method is deemed as vulnerable by
{\tool}, it will provide a {\em list of important statements as part
  of the program dependence graph (PDG) that are relevant to the
  detected vulnerability}. For example, it provides the partial
sub-graph of the PDG including the statements at the lines 13--15,
22--23, and 25--27 in Figure~\ref{fig:pdg} for the vulnerable code at
line 23 and line 27. We use the PDG sub-graph including important
statements to give developers the hints on the dependencies relevant
to the vulnerability for further investigation.


\input{text-gen}
