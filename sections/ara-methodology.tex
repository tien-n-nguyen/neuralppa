%\begin{itemize}
%    \item {\bf Task A.} Collect high-quality, compilable Java and C/C++ programs and extract PDG/CFGs.
%    \item {\bf Task B.} Design preliminary model to predict inter-statement control-flow and program dependencies.
%    \item {\bf Task C.} Implement model and design experiments for both qualitative and quantitative evaluation.
%    \item {\bf Task D.} Leverage automated vulnerability detection tool to identify vulnerable code fragments.
%    \item {\bf Task E.} Model testing, tool release, and manuscript dissemination.
%\end{itemize}

\noindent {\bf (T1) Benchmark Dataset Construction.} The ability of the intended \tool infrastructure to accurately predict control-flow and program dependencies depends on the precise ground-truth CFG/PDG edge information used to train the model. Since most static analysis tools generate false positives to a certain degree, we seek to design CFG/PDG combination strategies to create program-dependence graphbanks of varying quality. Each combination strategy can be associated with certain tradeoffs depending on the real-world scenario. For example, consider a strict combination strategy where only dependence edges generated by \underline{all} static analysis tools are selected. Such a graphbank would be useful to SE applications where each false positive is detrimental. Furthermore, the robustness of \tool infrastructure depends on whether the graphbanks are representative of different kinds of control-flow (e.g., \textit{sequential}, \textit{if-else}, etc.) and program dependence (\textit{data} and \textit{control}) edges. Thus, we plan to up/down sample programs in the graphbanks accordingly. Overall, we aim to create such benchmark datasets for both Java and C/C++.

\vspace{3pt}
\noindent {\bf (T2) \tool Infrastructure.} 
In the neural network-based dependency parsing approaches~\cite{chen-manning-2014-fast} in NLP, by projecting the words into an embedding space, researchers successfully learn the semantic relationships that model the dependencies between them. Thus inspired, we believe we can follow suit to learn the control-flow and program dependencies between the statements in programs as well. Moreover, given that the quality of the statement embeddings determines the accurate prediction of the program dependence relations, enhancing them by incorporating context from both within and across the statements is crucial. Intra-statement contextualization can help relay the local information within individual statements globally. For example, such information can help distinguishably identify the declaration and reference of the same variable across different program statements. In contrast, inter-statement contextualization helps learn the dependencies between the statements in the context of the surrounding statements. Finally, we aim to model the CFG/PDG construction as a pairwise dependence decoding task, wherein the combination of all the predicted edges in the statement pairs can be formalized as directed graphs, i.e., a program's CFG/PDG.

\vspace{3pt}
\noindent{\bf (T3) Fragment-Level Vulnerability Detection.} There are several deep learning (DL)-based VD tools, which input a CFG/PDG to detect the presence of vulnerabilities in code. These approaches are not directly affected by the incomplete nature of the code and are only trained on versions of the program representations. For example, consider VulCNN~\cite{wu2022vulcnn}, which inputs a program semantics-capturing image corresponding to a given CFG/PDG, and is modeled as an image classification system. Any such VD tool can be leveraged for the CFG/PDG predicted by \tool infrastructure for the given code fragment. 
