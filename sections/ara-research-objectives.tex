While the state-of-the-art research and practice has been well-established for deriving control-flow and program dependencies for entire programs, there have been little to no advancements in conducting such a program dependence analysis for partial programs. In this proposal, we set out to investigate and develop {\tool}, a {\em \underline{Neural} Network-Based \underline{P}rogram \underline{D}ependence \underline{A}nalysis} infrastructure, that enables (partial) program dependence analysis. Besides, leveraging one such neural network-based approach will help gain a significant time markup in dependency discovery in contrast to traditional program analysis tools.


\begin{center}
    \begin{minipage}{33.5em}
    The key philosophy that drives our work is that {\em the dependence analysis of partial code can be learned from the dependence analysis for entire programs via the wealth of information obtained from ultra-large-scale, open-source software repositories}.
    \end{minipage}
\end{center}


We draw motivation for such a data-driven, learning-based approach from the following. First, ultra-large-scale software repositories, e.g. GitHub (7M+ projects) and SourceForge (700k+ projects) contain an enormous collection of programs. These repositories amount to 1B+ lines of code, 10M+ revision logs, and 3M+ issue reports. This wealth of knowledge is an excellent source for {\tool}. Hindle {\em et al.}~\cite{} have shown that code has high repetitiveness and predictability, and can be captured well by statistical models. Thus, we expect to build ML/DL models to learn from those repositories. Second, in an empirical study on the repetitiveness, containment, and composability of PDGs in open-source projects, the PI group~\cite{} reported that among 17.5M PDGs with 1.6B PDG subgraphs, 14.3\% of the PDGs have all of their subgraphs repeated across different projects. Furthermore, in 15.6\% of the PDGs, at least 90\% of their subgraphs are likely to have appeared before in other projects. Thus, {\tool} could learn from PDGs with complete program dependencies retrieved from existing code repositories and derive the dependencies for the (partial) code fragment under study. Broadly, such a partial program dependence analysis infrastructure is similar in spirit to neural network-based dependency parsing approaches in natural language processing (NLP), which learn the dependencies signifying the semantic relationships between words in a sentence from the text corpora.

We propose the following thrusts of research:

\vspace{3pt}
\noindent \textbf{[Thrust 1] Lorem}

\vspace{3pt}
\noindent \textbf{[Thrust 2] Neural Network-Based Program Dependence Analysis Infrastructure.} The basic infrastructure has two main tasks. First, it learns the well-defined syntactic and semantic information in source code by leveraging the program representations obtained for the complete programs in the training dataset, collected from large-scale code repositories. Next, one such trained model infers the program dependencies among the statements in any given code fragment, either complete or partial code.

\vspace{3pt}
\noindent \textbf{[Thrust 3] Enabling Fragment-Level Vulnerability Detection with \tool.} Vulnerability detection in code fragments can be facilitated as follows: (a) employing \tool infrastructure to predict the CFG/PDG for a given code fragment; (b) leveraging an existing automated VD tool that utilizes the predicted CFG/PDGs to identify the presence of vulnerabilities in partial code fragments.
