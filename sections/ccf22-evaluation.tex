\section{Evaluation Plan}
\label{eval}

%Our {\em goals} of the evaluation plan include the studies to answer the questions:

%1) {\bf Intrinsic evaluation.} 

%2) {\bf Extrinsic evaluation.} How well do our proposed tools and methods help developers in improving the learning and usages of APIs in software libraries?

%3) How effectively do the proposed tools and methods help developers
%in real development processes?

\paragraph{{\bf (1) Comparative Studies - BD, FL, RT-CI and APR Approaches.}}
{\em We evaluate how well our CRL-powered approaches in bug
  detection (BD), fault localization (FL), Regression Testing in
  Continuous Integration (RT-CI) and automated program repair (APR)
  approaches are in comparison with the state-of-the-art approaches?}
First, we use the large-scale cross-language datasets with bug fixes.
To detect bugs, we will train our BD and FL approaches on buggy code
with known buggy locations and non-buggy code. For APR, we
train APR approaches on buggy code with its fixed versions to
automatically learn fix patterns. Second, for evaluation, we follow
prior research for BD, FL, and APR. \underline{In DB}, we will measure
the precision, recall, F-measure, and the number of true bugs detected
in Top-N (e.g., N=50). \underline{In FL}, we use the following:
\textit{Recall at Top-N}, the number of faults with at least one
faulty element located within the first N positions.  \textit{Mean
  Average Rank (MAR)}: For precise localization of all faulty elements
of each fault, we compute the average ranking of all the faulty
elements for each fault.  \textit{Mean First Rank (MFR)}: For each
project, we compute the mean of the first faulty element's rank for
each fault.  \underline{In RT-CI}, we calculate APFD (Average
Percentage of Faults Detected), Normalized APFD
(NAPFD)~\cite{qu2007combinatorial} and Normalized Rank Percentile
Average (NRPA)~\cite{bertolino2020learning}. Rank Percentile Average
(RPA) was proposed to adapt the Average Fault Percentile to the
prioritization problem for computing how much a predicted ranking is
close to the actual ranking.  \underline{In APR}, we calculate the
number of bugs automatically fixed and the ratio of correctly fixed
bugs to the number of plausible patches when comparing with
traditional APR approaches. Compared with DL ARP, we use: \textit{Top
  K is the number of times that a correct patch is in the ranked list
  of top K candidates, e.g., K=1,3,5}.

\paragraph{{\bf (2) Comparative Study on Code Representation Learning (CLR) for BD, FL, RT-CI and APR.}}
{\em Research question: How effective are our CRL for BD, FL, Testing and APR, and how well are they in comparison with the state-of-the-art approaches?} 
We will perform controlled experiments to evaluate the effectiveness of our CRL in BD, FL, RT and APR, especially ablation studies on different factors in CRL. In addition, we will continue to compare our CRL with other existing CRL on other software engineering and program analysis topics. We use the same evaluation metrics as the ones in (1) for BD, FL, and APR.

To evaluate our evaluation framework for CRL, we propose the following
procedure. First, we will choose a well-known DL-based model for each
application, BD, FL, RT-CI, and APR. For each of those DL-based
models, we varied the CRL model using our design framework in Thrust
1. We measure the quality of each CRL model and measure the
performance of the DL-based model for each application. We will use
statistical test to evaluate whether the high accuracy of CRL model
leads to high performance of the same downstream model used in the
application under study.

%\subsection{Evaluation on usefulness of the approaches in helping developers in learning API usages}

%We will perform controlled experiments to evaluate our methods and tools with developers in the loop using the actual system. We will
%gather developers and provide them various programming tasks, and
%compare the efficiency of their work with our methods/tools to the
%baseline approaches, e.g., with the Web-based code search or other
%code synthesis approaches.

%We will measure the efficiency of the work using different methods
%based on the following evaluation metrics. First, we measure
%efficiency. This could be measured by the time for developers to
%perform the tasks or the number tasks completed by developers in a
%period of time. Second, we measure coding effort: how much effort that
%developers must do to complete the API usages given the API code from
%the methods. Third, we will measure semantic correctness, either by
%the number of passing test cases or by human judgements. Finally, we
%will perform surveys on the quality of the synthesized API usage code
%to get the feedback for future improvement in the next step.

\paragraph{{\bf (3) Evaluation on proposed tools and methods in helping developers in real-world development}}

We will perform a set of user studies on the usefulness of our
proposed tools in the wild by releasing our
tools in the actual real-world development. 
%We will analyze all the aspects as in the previous studies and the total uptake of the tools.  
We will record feedback to improve our tools.
PI Wang will connect with the Industry through our \textit{The New Jersey Innovation Institute} (NJII)~\cite{njii}, an NJIT corporation focusing on helping private enterprise. NJII has a strong tie with companies like AT\&T, Facebook, and Bloomberg. PI Nguyen will work with his existing collaborators in Microsoft, IBM, ABB research.
%to perform user studies on the groups of developers in certain tasks to evaluate the usefulness of our proposed tools.
