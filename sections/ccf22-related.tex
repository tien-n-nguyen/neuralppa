\section{\color{red}{Related Work}}
%Our project is closely related to the following research literature: 
\textbf{Code Representation Learning (CRL).}  The recent success in
machine learning has lead to strong interests in applying machine
learning, especially deep learning, to programming language (PL) and
software engineering (SE) tasks, such as automated correction for
syntax errors~\cite{Bhatia-2016}, fuzz testing~\cite{Patra-2016},
program synthesis~\cite{Amodio-2017}, code
clones~\cite{White-2016,Smith-2009,Li-2017}, program
summarization~\cite{Allamanis-2016,Mou-2014}, code
similarity~\cite{Zhao-2018,Alon-2018}, probabilistic model for
code~\cite{Bielik-2016}, and path-based code representation,
e.g., Code2Vec~\cite{Alon-2018} and Code2Seq~\cite{alon2018code2seq}. 

%In the above PL and SE tasks, 
%All approaches learn code
%representations using different program properties. 
%Although not proposed for detecting bugs, they
%still very relevant to our proposal. %as one important step of our approach is to learn bug detection specialized code representation.

%Different from the existing CRL approaches, in our initial
%study~\cite{yioopsla19}, we conducted the CRL using the AST, DFG, PDG,
%and different attention-based neural networks. Our results show that
%our CRL is more suitable for detecting bugs than the existing baselines.


\textbf{Bug Detection.}
%Many techniques have been developed for rule-based and learning-based bug detection. 
Some existing rule-based bug detection approaches,
such as
\cite{Bian-2018,Jin-2012,Olivo-2015,Engler-2001,Cole-2006,Toman-2017},
are unsupervised. However, new rules are needed to
define to detect new types of bugs, e.g., FindBugs~\cite{Hovemeyer-2007} and NAR-miner~\cite{Bian-2018}.
%
%The most state-of-the-art \textit{rule-based} tool is NAR-miner~\cite{Bian-2018} that extracts
%negative code rules to detect bugs. %, instead of positive code rules. 
%Our initial results show the NAR-miner has a
%high false positive rate, i.e., 52\%, which make them
%impractical for daily use. 
%that it only costs NAR-miner 1 minute to perform bug detection on
%a project. However the mining-based approaches mainly suffer the
%problem of high false positive (FP) rates, such as the NAR-miner has a
%high FP rate, i.e., 52\% in the cross-project setting, which make them
%impractical for daily use. 
%When comparing our approach with the existing state-of-the art rule-based and mining approaches, the main differences are as follows. First, we consider the relations among paths from different methods for detecting cross-method bugs. However,
%they normally work on individual methods and cannot work well on the
%cross-method bugs. Second, our approach covers path information in an
%AST in order to detect very detailed bugs in each method, while the
%they often consider the important rules and may miss some information
%outside of their rules.
To overcome the pre-defined rules, the mining-based approaches, e.g.,~\cite{Bian-2018,Jin-2012,Olivo-2015,Engler-2001,Cole-2006,Toman-2017,Li-2005}, were proposed to mine implicitly programming rules from source code using data mining techniques. 
%Typically, this type
%of approaches extracts implicitly programming
%rules from program source code using data mining approaches
%(e.g., mining frequent itemsets or sub-graphs) and detects
%violations of the extracted rules as potential bugs. 
They still have a key limitation in a very high false positive rate
due to the fact that they cannot distinguish the cases of incorrect
code versus infrequent/rare code.  There exist DL-based
approaches~\cite{Pradel-2018,Wang-2016b} and traditional ML
techniques~\cite{Engler-2001,Li-2005,Wasylkowski-2017,
  Wang-2016b,Wang-2016,Liang-2016}.
%For example, 
%the Bugram \cite{Wang-2016} uses n-gram models to rank the methods and then picks the top-ranked methods as buggy methods.
%DeepBugs~\cite{Pradel-2018} uses deep learning techniques to propose
%a name-based bug detector.  
DL-based approaches are still limited to detect bugs in individual
methods.
%without investigating the dependencies among different yet
%relevant methods.

%$Due to that, they have high false positive rates, making them less
%practical in the daily use of software developers.
%In practice,
%there exist several cases that bugs occur across more than one
%method. That is, to decide whether a given method is buggy or not, a model
%needs to consider other methods that have data and/or control
%dependencies with the method under investigation. 
%Due to that, the
%existing DL-based approaches have high false positive
%rates, making them less practical in the daily use. % of software
%developers. 
%For example, DeepBugs~\cite{Pradel-2018} is reported to have a high false positive rate of 32\%. That is, approximately one out of 3 reported bugs is not a true bug, thus, wasting much developers' efforts. 
%Our study~\cite{yioopsla19i} also showed a false positive rate of 41\% for DeepBugs on our collected dataset.


%In this paper, our approach is also using the deep learning techniques to train the models and
%classify methods into buggy or non-buggy. Our approach is different
%from the existing learning-based approaches in the following ways.
%First, like the rule-based approaches, the existing learning-based
%approaches do not consider the relations among paths across multiple
%methods. In our code representation learning step, we model the
%relations among paths from different methods using the dependencies of
%entities in the PDG and DFG, in addition to the AST nodes of a path.
%Second, our approach uses long paths of an AST to cover all of the AST
%nodes for representing local context, while other existing approaches
%often use part of method information to detect bugs, such as
%name-based identifier representation and frequent $n$-grams. Our
%results show that our approach can outperform all of the
%studied baselines.


%\textit{Machine learning-based bug	detection~\cite{Wasylkowski-2017,Wang-2016b,Wang-2016,Pradel-2018}.}
%With the advances of machine learning (ML) and especially deep
%learning models, several approaches have been proposed to learn from
%previously known and reported bugs and fixes to detect bugs in the
%new code. While the ML-based bug detection models~\cite{Nam-2015}
%rely on feature selections, the deep learning-based
%ones~\cite{Pradel-2018,Wang-2016b} take advantages of the capability
%to learn the important features from the training data for bug
%detection. 


\textbf{Fault Localization and Regression Testing.}
Spectrum-based Fault Localization (SBFL)~\cite{zhang2011localizing, abreu2007accuracy, jones2005empirical, abreu2006evaluation, naish2011model, wong2007effective, liblit2005scalable, lucia2014extended, keller2017critical} has been intensively studied in the literature, e.g., Ochiai \cite{abreu2006evaluation} and Jaccard \cite{abreu2007accuracy}. 
%Tarantula \cite{jones2001visualization}, SBI \cite{liblit2005scalable}, Ochiai \cite{abreu2006evaluation} and Jaccard \cite{abreu2007accuracy}.  
%they share the same basic insight, i.e., code elements mainly executed by failed tests are more suspicious. 
Mutation-based Fault Localization (MBFL)~\cite{moon2014ask, zhang2013injecting,budd1981mutation, zhang2010test, musco2017large} aims to additionally consider impact information for fault localization, e.g., Metallaxis \cite{papadakis2012using, papadakis2015metallaxis} and MUSE \cite{moon2014ask}.
%Since code elements covered by failed/passed tests may not have any impact on the corresponding test outcomes, e.g., Metallaxis \cite{papadakis2012using, papadakis2015metallaxis} and MUSE \cite{moon2014ask}.
%typical MBFL techniques
%use mutation testing \cite{budd1981mutation, zhang2010test, musco2017large} to simulate the impact of each
%code element for more precise fault localization, e.g., Metallaxis \cite{papadakis2012using, papadakis2015metallaxis} and MUSE \cite{moon2014ask}.
%The first general MBFL technique, Metallaxis [24, 26] is based on the
%following intuition: if one mutant has impacts on failed tests (e.g.,
%the tests outcomes change after mutation), its corresponding code
%element may have caused the test failures
%
%The more recent MUSE [23] technique has similar insights: (1)
%mutating faulty elements may cause more failed tests to pass than
%mutating correct elements; (2) mutating correct elements may
%cause more passed tests to fail than mutating faulty elements.
%
%Learning-to-Rank (LtR) has been used to improve fault localization~\cite{b2016learning, xuan2014learning, li2017transforming, sohn2017fluccs}.
%MULTRIC \cite{xuan2014learning} combines different suspiciousness values from SBFL. Some work combines SBFL suspiciousness values with
%other information, e.g., program invariant \cite{b2016learning} and source code complexity information \cite{sohn2017fluccs}, for more effective LtR fault localization. 
%TraPT \cite{li2017transforming} combines suspiciousness values from both SBFL and MBFL. 
Neural networks have been applied to fault
localization~\cite{zheng2016fault, briand2007using, zhang2017deep,
  wong2009bp}. However, they mainly work on the test coverage
information, which has clear limitations (e.g., it cannot distinguish
elements accidentally executed by failed tests and the actual faulty
elements)~\cite{li2017transforming}. DeepFL~\cite{DeepFL},
deep-learning based, was proposed to improve method-level FL, and it
improves the learning-to-rank FL approach,
TraPT~\cite{TraPT}. Extensive research has been on test case selection and prioritization in regression testing (RT), e.g., coverage-based~\cite{di2015coverage}, search-based~\cite{de2011multi,yu2010time}, historical tests~\cite{kim2002history,marijan2013test,noor2015similarity,park2008historical}, code dependencies~\cite{gligoric2015ekstazi} and information retrieval~\cite{kwon2014test,saha2015information}. 
%RT in Continuous Integration (CI) differs from the traditional RT as 
However, most techniques cannot be applied at the scale of modern Continuous Integration (CI) practices~\cite{elbaum2014techniques} that require frequent commits to the shared code base and cause the cost of continuous RT escalate dramatically~\cite{memon2017taming}. For example, coverage information may be infeasible and imprecise to collect at large scale and in quick cycles and code changes~\cite{elbaum2014techniques,haghighatkhah2018test,hemmati2015prioritizing,yu2018study}. 
Recently, RT in CI has been investigated, e.g.,~\cite{elbaum2014techniques,gligoric2015practical,yu2018study,jiang2009adaptive,henard2016comparing}. 
However, existing approaches, e.g.,~\cite{bertolino2020learning,spieker2017reinforcement} are still limited in accuracy and scalability as they lack the learning of deep features of test cases and source code and their relations. 
%Moreover, very little research has been devoted to investigating deep learning for RT in CI. 


\textbf{Automated Program Repair.}
%The earlier approaches for automated program repair (APR) aim to derive {\em similar fixes for similar source code}. Ray and Kim~\cite{ray-fse12} automatically detect similar fixes for similar code that are ported or branched out. FixWizard~\cite{icse10} automatically derives {\em the fixes for similar code} that were cloned from one place to another.
APR approaches have explored search-based SE to tackle more general
types of bugs, e.g., GenProg~\cite{le2011genprog},
RSRepair~\cite{qi2014strength}, MutRepair~\cite{martinez2016astor},
PAR~\cite{kim2013automatic}, and SPR~\cite{long2015staged}.
%\cite{le2011genprog,qi2014strength,LeGoues-icse12,martinez2016astor}.
%A search strategy is performed in the space of potential solutions
%produced by several operators that mutate the buggy code. Then, the
%test cases and/or program verification are applied to select the
%better candidate fixes~\cite{smith2015cure}. 
%Smith et al.~\cite{smith2015cure} showed that 
%However, these
%approaches tend to overfit on test cases, by generating incorrect
%patches that pass the test cases, mostly by deleting functionalities~\cite{smith2015cure}.
%
%GenProg~\cite{le2011genprog} uses genetic search on repair mutations
%and works at statement-level by inserting, removing or replacing a
%statement taken from other parts of the same
%program. RSRepair~\cite{qi2014strength} fixes buggy programs with
%random search to guide the patch generation
%process. MutRepair~\cite{martinez2016astor} attempts to generate
%patches by applying mutation operators on suspicious if-condition
%statements. 
%PAR~\cite{kim2013automatic} is based on {\bf fixing templates} that
%were manually extracted from 60,000 human-written patches. 
%Later
%studies (e.g., \cite{le2016history}) showed that the six templates in PAR~\cite{kim2013automatic} could fix only a few bugs in Defects4J. 
%Anti-patterns were integrated into search-based APR tools~\cite{tan2016anti}
%Anti-patterns were integrated into search-based APR tools, e.g., GenProg~\cite{le2011genprog} and SPR~\cite{long2015staged}), to help alleviate the problem of incorrect or incomplete fixes.
%Theses search-based approaches rely much on the quality of mutation operations and the fixing patterns.
%
%\textbf{Hard-coded Rules based APR.}  Some existing APR techniques are
%mostly based on hard-coded rules and operators, defined by human
%experts, used to generate potential patches obtained with mutation
%operators for a buggy code.  For example, GenProg~\cite{le2011genprog}
%uses genetic search on repair mutations and works at statement-level
%by inserting, removing or replacing a statement taken from other parts
%of the same program.  RSRepair~\cite{qi2014strength} tries to fix
%buggy programs with the same mutation operations in GenProg but uses
%random search to guide the patch generation process.
%MutRepair~\cite{martinez2016astor} attempts to generate patches by
%applying mutation operators on suspicious if-condition statements.
%These approaches have several limitations, rooted in the fact that
%they are based on handcrafted rules with limited scope (i.e., single
%statements, limited number of mutation operators, specific parts of
%expressions).  Moreover, Smith et al.~\cite{smith2015cure} showed that
%these approaches tend to overfit on test cases, by generating
%incorrect patches that pass the test cases, mostly by deleting
%functionalities.
%
%Kim et al.~\cite{kim2013automatic} initiated with PAR a milestone of APR based on fix templates that were manually extracted from 60,000 human-written patches. Later studies (e.g., \cite{le2016history}) have shown that the six templates used by PAR could fix only a few bugs in Defects4J.
%~\cite{tan2016anti} manually define a set of patterns for patch generating or filtering based on repair history.
%Anti-pattern~\cite{tan2016anti} integrated anti-patterns into two existing
%search-based automated program repair tools (namely, GenProg~\cite{le2011genprog} and SPR~\cite{long2015staged}) to help alleviate the problem of incorrect or incomplete fixes resulting from program repair.
%In their study, the anti-patterns are defined by themselves and limited
%to the control flow graph. Additionally, their anti-patterns are
%not meant to solve the problem of deriving better patches
%automatically, provide more precise repair hints to developers.
%In contrast to search-based approaches, other approaches have aimed to
%Another type of APR approaches {\bf mine and learn fixing patterns} from prior bug
%fixes~\cite{nguyen2013semfix,le2016history,liu2019avatar,kim2013automatic}, \textit{generating the state-of-the-art performance}. 
Another type of APR approaches {\bf mine and learn fixing patterns} from prior bug
fixes, \textit{generating the state-of-the-art performance}. The fixing patterns (or namely fixing templates) could be automatically
or semi-automatically mined, such as SemFix~\cite{nguyen2013semfix}, Prophet~\cite{long2016automatic}, Genesis~\cite{long2017automatic}, FixMiner~\cite{koyuncu2018fixminer}, HDRepair~\cite{le2016history}, CapGen~\cite{wen2018context}, SimFix~\cite{jiang2018shaping}, Avatar~\cite{liu2019avatar}, Nopol~\cite{Nopol}, ACS~\cite{ACS}, SketchFix~\cite{SketchFix}, ELIXIR~\cite{saha2017elixir}, and LSRepair~\cite{LSRepair}. Tbar~\cite{tbar-issta19} collects all existing fix-patterns.
%exploit fix patterns of static analysis
%violations as ingredients for patch generation.
%
%SemFix~\cite{nguyen2013semfix} uses symbolic execution and
%constraint solving to synthesize a patch. 
%Some studies extracts fix patterns from real human submitted patches or change histories, e.g., Prophet~\cite{long2016automatic}, Genesis~\cite{long2017automatic}, FixMiner~\cite{koyuncu2018fixminer}, and HDRepair~\cite{le2016history}.
%
%are based on the frequently
%occurred code change operations (e.g., Insert If- Statement) extracted from the patches in developer change histories.
 %
%Prophet~\cite{long2016automatic} learns code
%correctness models from a set of successful human patches. 
%Prophet learns a patch ranking model using machine learning algorithm based on
%existing patches. 
%Genesis~\cite{long2017automatic} can automatically
%infer patch generation transforms from developer submitted patches for APR.
%HDRepair~\cite{le2016history} repairs bugs by mining closed frequent bug fix patterns from
%graph-based representations of real bug fixes.
%ELIXIR~\cite{saha2017elixir} uses method call related templates from
%PAR with local variables, fields, or constants, to construct more
%expressive repair-expressions for synthesizing
%patches. 
%CapGen~\cite{wen2018context}, SimFix~\cite{jiang2018shaping},
%Avatar~\cite{liu2019avatar} exploit fix patterns of static analysis
%violations as ingredients for patch generation.
%\textbf{Fix Pattern Mining and Learning based APR.}  Another direction
%of APR approaches tend to automatically mine fix patterns or
%templates.  For example, SemFix~\cite{nguyen2013semfix} instead uses
%symbolic execution and constraint solving to synthesize a patch by
%replacing only the right-hand side of assignments or branch
%predicates.  Long and Rinard also proposed a patch generation system,
%Prophet~\cite{long2016automatic}, that learns code correctness models
%from a set of successful human patches. Prophet learns a patch ranking
%model using machine learning algorithm based on existing patches.
%They further proposed a new system, Genesis~\cite{long2017automatic},
%which can automatically infer patch generation transforms from
%developer submitted patches for automated program repair.  Motivated
%by PAR~\cite{kim2013automatic}, more effective automated program
%repair systems have been explored. HDRepair~\cite{le2016history} was
%proposed to repair bugs by mining closed frequent bug fix patterns
%from graph-based representations of real bug fixes.  Nevertheless, its
%fix patterns, except the fix templates from PAR, still limits the code
%change actions at abstract syntax tree (AST) node level, but are not
%specific for some types of bugs.  ELIXIR~\cite{saha2017elixir}
%aggressively uses method call related templates from PAR with local
%variables, fields, or constants, to construct more expressive
%repair-expressions that go into synthesizing patches.  More recently,
%CapGen~\cite{wen2018context}, SimFix~\cite{jiang2018shaping},
%FixMiner~\cite{koyuncu2018fixminer} are further proposed to fix bugs
%automatically based on the frequently occurred code change operations
%(e.g., Insert If- Statement) that are extracted from the patches in
%developer change histories.  Avatar~\cite{liu2019avatar} exploits fix
%patterns of static analysis violations as ingredients for patch
%generation So far however, pattern-based APR approaches focus on
%leveraging patches that developer applied to semantic bugs.
%\textbf{Deep Learning based APR:} 
Recently, deep learning has
been applied to APR for directly generating patches.  
%For example, DeepRepair~\cite{white2016deep} and DeepFix~\cite{gupta2017deepfix} learn similar source code for similar fixes.
%The first group
%of DL-based APR approaches {\em learn similar source code for similar fixes}. such as DeepRepair~\cite{white2016deep} and DeepFix~\cite{gupta2017deepfix}.
%DeepRepair
%leverages learned code similarities, captured with recursive
%auto-encoders~\cite{white2016deep}, to select the repair ingredients
%from code fragments that are similar to the buggy code.
%DeepFix~\cite{gupta2017deepfix} learns the syntax rules and is
%evaluated on syntax errors.
Some work treats APR as a {\em statistical machine translation} translating the buggy code to the fixed
code. A natural thinking is to use neural translation models to learn to recommend fixes, such as Ratchet~\cite{hata2018learning}, Tufano {\em et al.}~\cite{tufano2018empirical}, SequenceR~\cite{chen2018sequencer}, Tufano {\em et al.}~\cite{tufano2019learning}, CoCoNuT~\cite{lutellier2020coconut} and CODIT~\cite{CODIT}. 


%learns code edits with encoding code structures in
%an NMT model to recommend fixes. 

%learn code changes using S2S NMT with code abstractions and keyword
%replacing.

%use sequence-to-sequence (S2S)
%translation. %They use neural network machine translation (NMT) with attention-based Encoder-Decoder, and different code abstractions to
%generate patches, while 
%SequenceR~\cite{chen2018sequencer} uses S2S with a copy mechanism~\cite{see2017get}.
%CODIT~\cite{codit} learns code edits with encoding code structures in
%an NMT model to recommend fixes. 
%The comparison with these NMT-based
%APR approaches is provided in the introduction. 
%Tufano {\em
%	et al.}~\cite{tufano2019learning} learn code changes using S2S NMT with code abstractions and keyword
%replacing. 
%Despite of treating the APR as code transformation learning
%problem, their approach takes entire method as a context for a bug.
%Thus, it has too much noise, leading to lower effectiveness than our initial approach {\tool}~\cite{icse_fl}.

%than {\tool}. In other words, the treatment of context from {\tool} helps improve over their model.

\begin{comment}
%There exist a rich research literature in code search, code
%recommendation/suggestion, and program synthesis. They can be broadly
%classified into the following categories.

Our work is closely related to the {\bf statistical NLP approaches for
  generating code from text.}
%A related work to {\tool} is SWIM~\cite{Raghothaman-ICSE16} to
%generate code from English text.
SWIM~\cite{Raghothaman-ICSE16} follows a phrase-based SMT approach
that first uses IBM Model to produce code elements and then uses
$n$-gram model to synthesize a code sequence. In comparison, after IBM
Model, we expand the set of APIs by using context expansion, which
could produce better results. We also use graph synthesis, which is
suitable for API usages than $n$-gram.
%Their tools are not available for comparison.
Desai {\em et al.}~\cite{Desai2016ICSE} synthesize domain-specific
languages from English. Instead of using alignment methods, a user
is required to map key terms in English to the terminals in the DSL.

DeepAPI~\cite{gu-fse16} uses Recurrent Neural Network to generate API
usage sequences for a given text by using deep learning to relate
APIs. In contrast, we use graph-based translation with graph
synthesis. DeepAPI uses deep learning on sequences. In Ye {\em et
  al.}~\cite{ye-icse16}, the authors used Skip-gram model on API
documentation/tutorials to produce embeddings to relate words and API
elements.
%Semantic similarities among such documents are modeled via those
%embeddings.
They focus on improve code retrieval from texts, rather than generating new
code. 
%
Allamanis {\em et al.}~\cite{bimodal15} introduce a jointly
probabilistic model short natural language utterances and code
snippets.~In comparison, they have a joint model with a
tree-based~representation used for code and texts. Our work uses
graph structures.
%Second, while their approach uses advanced
%\emph{bimodal modeling} (e.g., image+text, text+code), we treat code
%synthesis as a machine translation problem, allowing different
%language models for texts and source code.
TBCNN~\cite{tbcnn14} uses tree structures to suggest next code
tokens.
%
Anycode~\cite{anycode-oopsla15} uses a probabilistic CFG with trees
for Java constructs and API calls to synthesize small Java
expressions. Maddison and Tarlow~\cite{tarlow14} present a generative
model for source code, which is based on AST-based syntactic
structures.
%In comparison those two approaches,
We use graphs to capture better program dependencies. Other NLP
approaches have been used to support
%Statistical learning has been used in SE. They include
code~suggestion ~\cite{hindle-icse12,white-msr15}, code
convention~\cite{barr-codeconvention-fse14}, name
suggestion~\cite{bird-fse15}, API suggestions~\cite{raychev-pldi14},
large-scale code mining~\cite{sutton-msr13}, etc.

%\vspace{0.03in}
%\noindent {\bf Information retrieval (IR) approaches.}

Other approaches use {\bf program analysis to synthesize code}. Buse
and Weimer~\cite{buse-icse12} use path sensitive dataflow analysis,
clustering, and pattern abstraction to synthesize API usages from code
examples. They do not handle {\em textual queries}.
%It does not synthesize {\em new} API usages from the smaller ones. It
%generalizes concrete code into more general/succinct API usages.
%Sourcerer~\cite{sourcerer-oopsla06} exploits structural and usage
%relations to rank candidates.
Others explore structure relations~\cite{sourcerer-oopsla06}, call
graphs (FACG~\cite{zhang-fase11}), program dependencies
(MAPO~\cite{mapo-ecoop09}, Altair~\cite{altair-fse09}), or
input/output types~\cite{jungloid-pldi05}.
%MAPO mines and recommends API usage patterns and does not analyze
%textual queries with statistical learning as in {\tool}.
Altair~\cite{altair-fse09} and FACG~\cite{zhang-fase11} suggest the
similar APIs to the function in the query using common functions and
weighted API call graph respectively.
%Prospector~\cite{jungloid-pldi05} synthesizes code for a query with
%input/output types.

%is a technique for synthesizing jungloid code fragments automatically
%given a simple query for input/output types.

%Finally, 85\% of the synthesized graphs (not shown) do not exist
%as a whole in the training data. Thus, this shows T2API¡¯s capability
%to generate new graphs from smaller already-seen subgraphs

While {\em our work generates new API usage from text}, {\bf
  code search approaches} mainly use IR approaches to improve accuracy
of retrieving existing
code~\cite{openhub,codase,inoue-icse03,sourcerer-oopsla06,gridle-sac06,sawadsky-icse13,brandt-chi10}.
%In our evaluation, {\bf 85\% of the synthesized graphs do not exist as
%  a whole in the training data, thus, cannot be found by code search}.
%. Traditional code search engines use IR
%techniques in text
%matching~\cite{openhub,codase,inoue-icse03,sourcerer-oopsla06,gridle-sac06}.
%(Black Duck Open Hub~\cite{openhub}, Codase~\cite{codase}) often use
%text matching.  Other IR-based approaches allow users to use natural
%language texts as a query and match using keywords on
%programs~\cite{inoue-icse03,sourcerer-oopsla06,gridle-sac06}.
%Others enhance IDE with code search~\cite{sawadsky-icse13,brandt-chi10}.
%components~\cite{inoue-icse03}, and program
%structures (e.g., Sourcerer~\cite{sourcerer-oopsla06},
%Gridle~\cite{gridle-sac06}). Other research enhances IDEs with
%searching capabilities on code-related web
%pages~\cite{sawadsky-icse13,brandt-chi10}.
Other IR-based code search approaches consider the relations
among API elements~\cite{zheng-fse11,examplar-icse10,prem-fse07,gridle-sac06}.
%Random walks~\cite{prem-fse07} and PageRank~\cite{gridle-sac06} have
%been used in considering how methods in classes are called and used to
%support method searching.
%Zheng {\em et al.}~\cite{zheng-fse11} mines search results of Web
%search engines to recommend related APIs of different
%libraries.
McMillan {\em et al.}~\cite{McMillan-rsc10} locate a
set of APIs that are textually similar to the query and find
code examples cover them.
%Exemplar~\cite{examplar-icse10} takes a textual query and uses IR and
%program analysis to retrieve relevant
%applications.
Portfolio~\cite{portfolio-icse11} examines the context of call
graphs.
% when taking textual queries.
%Refoqus~\cite{haiduc-icse13} is trained in a sample of queries and
%relevant results and automatically recommends a reformulation strategy
%for a given text query to have better retrieval accuracy.
Chan {\em et al.}~\cite{chan-fse12} find connected API graphs with nodes having high
textual similarity to the query.
%In comparison, {\tool} learns the relevant API elements via
%statistical learning (without textual matching) and ensembles them via
%graph-generative language model trained from a large corpus, thus, can
%synthesize new API usages.



%Li and Ernst~\cite{li-icse12} detects buggy code similar to a given
%buggy code fragment via PDG.

%+ structural info relations

%+ usage info

%+ control flow dependencies

%+ MAPO, Propector, Altair, FACG,

%\vspace{0.03in}
%\noindent {\bf Constraint-based approaches.}

%\noindent {\bf Contraint-based code search.}
Semantic code search approaches aim to match constraints given as
input.
%XSnippet~\cite{xsnippet-oopsla06} supports context-sensitive retrieval
%for object instantiation.
%However, their context is the code under editing, while in {\tool}, a
%context is the words in the query and the corresponding code
%elements.
%PRIME~\cite{mishne-oopsla12}
%statically mines and consolidates
%temporal API specifications from partial programs in the form of code
%snippets.
%PRIME~\cite{mishne-oopsla12} matches a query against temporal
%specifications and their corresponding code snippets. In Stolee {\em
%  et al.}~\cite{stolee-tosem14},
%a code repository contains programs encoded as constraints and
%a constraint solver is used to find encoded programs that match an
%input/output query.
%Later, symbolic execution is used to further
%improve the results~\cite{stolee-jss15}.
They use context-aware retrieval~\cite{xsnippet-oopsla06}, temporal
specifications~\cite{mishne-oopsla12}, constraint
solver~\cite{stolee-tosem14}, symbolic execution~\cite{stolee-jss15},
formal logic and theorem prover~\cite{penix-ase99}, test case
execution~\cite{codegenie07,reiss-icse09}, control flow analysis in
Parseweb~\cite{parseweb-ase07}.
%,zaremski-tosem97}.
%Other search engines execute test cases on candidate
%code~\cite{codegenie07,reiss-icse09}.
%Tien ccf19
%Parseweb~\cite{parseweb-ase07} uses control flow analysis to suggest
%API calls for a query with the input and output~types.
%It is able to compose smaller code to form more complex code
%fragments.
%In comparison, {\tool} is statistical, while Prospector and Parseweb
%rely on program analysis to compose code satisfying the types.

%\begin{comment}
%Tu \etal~\cite{tu-fse14} use a $n$-gram based cache for code
%suggestions. They suggestion a single ``next'' token given a preceding
%sequence of tokens. They achieve a top 1 suggestion of 61\%. Since
%they suggest only one token, their precision and recall are
%equivalent. These suggestions come from the preceding 2k tokens in the
%same project as well as a general n-gram model. In contrast, we
%suggest entire code snippets from English and use graphs to synthesize
%new code instead suggesting existing n-grams. Our precision is 2
%points higher and our recall results are 23 points higher than their
%suggestion. We are suggesting entire code snippets with an average of
%13 code elements not a one ``next'' token.
%\end{comment}



%-------------------------------------------------------

% Tien
\vspace{0.03in}
\noindent {\bf Domain-specific code synthesis.}  Typical applications
for such synthesis are string analysis~\cite{gulwani-11}, bit vector
processing~\cite{jha-icse10,Solar-LezamaTBSS06}, structure
manipulation~\cite{singh-fse11}, finite sketch programs~\cite{Solar-Lezama-pldi07,Solar-Lezama-pldi08}, spreadsheet
transformations \cite{harris-pldi11}, geometry
constructions~\cite{gulwani-pldi11}, and hardware
design~\cite{raabe-dac09}. In comparison,
we use machine translation.
%-----------------------------------------------------------

%They do not handle textual queries.

%Moreover, many of them can not be generalized for general purpose API
%libraries.




%Researchers used simple regular expressions that identified terms by
%Camelcase, \eg {\tt AccountManager} is a class. While a slight
%improvement over IR approaches the precision and recall remained low
%$0.33$ and $0.64$ \cite{Bacchelli2010ICSE}. RecDoc used a number of
%sophisticated resolution techniques including term context to attain a
%high precision and recall above $0.90$ \cite{Dagenais2012ICSE}.
%Performance issues made it impossible for RecDoc to parse large
%document corpora.  In a recent work, Subramanian
%\etal suggest a technique that can only parse code snippets and misses
%API elements that are in freeform text \cite{Subramanian2014ICSE}.


\end{comment}
