\subsection{Research Objectives and Anticipated Results}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.83\textwidth]{graphs/neuralppa}
    \vspace{-12pt}
    \caption{{\tool}: Machine Learning-based Program Analysis Infrastructure}
    \label{fig:arch}
\end{figure}


While the state-of-the-art research and practice has been
well-established for the analysis of the entire programs, very little
research and knowledge has been achieved for partial program
analysis.
%
In this proposal, we set out to investigate and develop {\tool}, a
{\em \underline{Neural}-network-based \underline{P}artial
  \underline{P}rogram \underline{A}nalysis infrastructure}. We aim to
develop {\bf a scientific foundation, novel methodologies, frameworks,
  models, and algorithmic solutions for neural partial program
  analysis}. {\tool} {\bf enpowers program analysis (PA) with advanced
machine learning (ML) and artificial intelligence (AI) to enable the
program analysis on incomplete code fragments} (Figure~\ref{fig:arch}).
{\tool} will allow the constructions of the program analysis
techniques for partial code on which downstream software
engineering applications can be built.


In this work, our key philosophy is that {\em the analysis of
  partial code can be learned from the analysis of entire programs in
  the wealth of information from ultra-large-scale, open-source
  software repositories}.



Specifically, we draw our motivation for such a data-driven,
learning-based approach from the following. First, ultra-large-scale
software repositories, e.g. GitHub (7M+ projects) and SourceForge
(700k+ projects) contain an enormous collection of programs. These
repositories amount to 1,000,000,000+ lines of code, 10,000,000+
revision logs, and 3,000,000+ issue reports. This wealth of knowledge
is an excellent source for {\tool}. Hindle {\em et
  al.}~\cite{naturalness-icse12} have shown that code has high
repetitiveness and predictability, and can be captured well by
statistical models. Thus, we expect to build ML models to learn from
those repositories. Second, in an empirical study on the
repetitiveness, containment, and composability of PDGs in open-source
projects, the PI group~\cite{msr16} reported that among
17.5M PDGs with 1.6B PDG subgraphs, 14.3\% of the PDGs have all of
their subgraphs repeated across different projects. Furthermore, in
15.6\% of the PDGs, at least 90\% of their subgraphs are likely to
have appeared before in other projects. Thus, {\tool} could learn from
PDGs with complete program dependencies retrieved from existing code
repositories and derive the dependencies for the (partial) code
fragment under study. The PI group also reported a high repetitiveness
level for AST code structure in open-source
projects~\cite{icse15}.

Finally, such a program analysis infrastructure like {\tool} can be
drawn from the spirit and successes of the approaches in natural
language processing (NLP). For example, at the lexical level, the task
of deriving the token types for source code tokens could be analogous
to the part-of-speech (PoS) tagging in NLP. At the syntax level, the
task of learning the syntactic structure in AST of the partial code
can be inspired by the approaches to build parse trees for
natural-language texts. At the semantic level, the partial program
dependence analysis infrastructure is similar in spirit to the the
neural network-based dependency parsing in NLP, which learns the
dependencies signifying the semantic relationships between words in a
sentence from text corpora.




In {\tool}, we propose the following thrusts of research
(Table~\ref{tab:milestones}):

\vspace{3pt}
\noindent \textbf{Thurst 1. Neural Structural Analysis Infrastructure
  \code{NeuralStruct}.} ({\em Section~\ref{}}) Source code has
well-defined structures and semantics. Thus, the basic infrastructure
in {\tool} is the neural structural analysis (\code{NeuralStruct})
component.  This component has two main tasks. First, it learns from
the syntactic structures of the complete code in the training dataset
collected from large-scale code repositories, to derive the abstract
syntax tree (AST) that best represents the syntactic structure of the
given partial code, i.e., with the highest likelihood/probability.
The traditional lexical analyzer still works for partial code due to
the independence nature of lexical tokens. The second task of this
component is to tag the code tokens with the types of the syntactic
units including the statement types (\code{if}, \code{for}, etc.),
variables, fields, methods, classes, etc. Both of the tasks can be
performed with our learning-based approaches in a dual-learning
manner.
  
\vspace{3pt}
\noindent \textbf{Thurst 2. Neural Semantic Analysis Infrastructure.}
({\em Section~\ref{}}) The basis components for several program
analysis techniques include the following:

1) the identification of the APIs of the external libraries in the
external references in the partial code: this is needed because the
partial code contains the undeclared reference and/or
declaration/reference ambiguity without explicit declaration of the
APIs in the external libraries.

2) the inference of the type information for the entities in the
partial code: due to the ambiguity in the declaration, the types of
the variables and statements are not always obviously defined. Thus,
the type inference is a basic service within {\tool}.

3) the inference of the program dependencies among the statements in
the partial code: several program analysis techniques are based on the
program dependencies, which are not always obtainable due to the
incompleteness of the given code fragment.

\vspace{3pt}
\noindent \textbf{Thurst 3. Neural Symbolic Execution Infrastructure.}
...

\vspace{3pt}
\noindent \textbf{Thurst 4. Neural Partial Program Analysis
  Applications.}  ({\em Section~\ref{}}) Our last thrust of research
is aimed to evaluate our basic partial program analysis infrastructure
in a few applications. We choose the following software engineering
applications: 1) software vulnerability detection for code snippets,
2) fault localization, and 3) code completion.

\vspace{3pt}
\noindent \textbf{Thurst ???. Neural Execution Analysis Infrastructure.}
({\em Section~\ref{}}) All the dynamic analysis techniques require the
analysis and understanding of the execution. However, for an
incomplete code, we first need to design a component that can wrap
around the given code fragment with the minimum code so that the code
fragment can be executed. When the code is executed, we also need the
approaches that represent the executed statements and their relations,
model the execution and stack traces, and model the code coverages
for an execution.


\input{sections/ccf22-plan-table}

Toward this theme, in our preliminary work, we developed DeepPDA, a
neural network-based partial program dependence analysis approach that
learns to derive the program dependencies for any code fragments
(i.e., both complete and incomplete). In our preliminary empirical
evaluation, we intrinsically evaluated it on Java and C/C++
programs. First, we trained {\tool} on complete code. For testing, we
treated each method individually and chose a consecutive portion
within the method to predict the program dependencies, and compared
them against the actual dependencies. Overall, DeepPDA predicts CFG
and PDG edges in Java with an F-score of 94.29\%, and in C++ with an
F-score of 92.46\%. We also developed as an approach to derive the
data types of the variables in the code snippets. We treat the problem
as statistical machine translation from source code with partially
qualified names to source code with FQNs of the APIs. Our empirical
evaluation on real-world code and StackOverflow posts shows that our
technique achieves high accuracy with 97.6\% precision and 96.7\%
recall in deriving data types in code snippets.
