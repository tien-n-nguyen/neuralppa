\section{On-going Work on ML-based Vulnerability Detection for Code Snippets}
\label{sec:vd}


%\section{Method-Level Vulnerability Detection}
%\label{mlvd:sec}

%\subsection{Problem Formulation}

Deep learning (DL)-based approaches that utilize PDGs for
vulnerability detection (VD) can tolerate a low level of errors in the
program dependencies, wherein the imprecision acts as noise and aids
in regularizing the model. VulCNN~\cite{wu2022vulcnn} is one such
state-of-the-art method-level VD tool that takes as input a program
semantics-capturing image extracted from the PDGs. In our plan,
we seek to determine how the PDGs predicted by \tool (say,
PDG\textsuperscript{*}) for {\em complete} methods affect the
performance of downstream tasks.

%We will describe another VD experiment
%for code snippets in Section~\ref{sec:fragment}.

We leverage VulCNN by taking as input both PDG\textsuperscript{*} and
the PDG derived from a program analysis tool (say,
PDG\textsuperscript{\#}) for these methods, aiming to see how closely
PDG\textsuperscript{*} mimics PDG\textsuperscript{\#} and approximates
the performance of the VD model. Mathematically, we can formulate our
task as follows:
\begin{equation}
    \centering
    0 < VD\{PDG\textsuperscript{*}\} \leq VD\{PDG\textsuperscript{\#}\}
\end{equation}
where $VD\{.\}$ indicates the performance of the automated VD model. Here, if $VD\{PDG\textsuperscript{*}\} \lesssim VD\{PDG\textsuperscript{\#}\}$, we can establish the efficacy of the PDGs predicted by \tool for downstream SE tasks such as vulnerability detection.


\subsubsection*{\bf Data Collection}
We facilitate this study by using the VD dataset collected by Li {\em
  et al.}~\cite{yioopsla19}, which comprises of
%+4.9 million 
complete Java methods collected from eight large
open-source Java projects. First, we filter by projects, dedicating
\textit{avro}, \textit{camel}, \textit{hbase}, \textit{hive},
\textit{lucene-solr}, and \textit{pig} for training purpose;
\textit{flink} and \textit{cloudstack} for validation and testing
respectively.
%We retain all methods that have between 3 and 10
%statements in each of these splits.
Finally, we randomly select an equal number of data samples from the
vulnerable and non-vulnerable method subsets in each of the splits, to
obtain about 8K methods for training and about 1K each for testing and
validation.

\subsubsection*{\bf Experiment Setup}
For all Java methods in the~VD dataset, we extract program
dependencies (i.e., data and control-dependence edges) via Joern
program analysis tool~\cite{joern-2014}. Next, we leverage \tool that
was trained on a Java dataset
%in Section~\ref{sec:effectiveness}
%(Table~\ref{tab:intrinsic-java})
to generate the PDGs (i.e., PDG\textsuperscript{*}) for all the
complete methods in the VD dataset. We then used
PDG\textsuperscript{\#} and PDG\textsuperscript{*} as the input of
VulCNN for VD.
%VulCNN leverages centrality analysis to transform the PDGs into program semantics-capturing images. Following suit, we generate two image datasets corresponding to PDG\textsuperscript{\#} and PDG\textsuperscript{*}, each of which are input to a convolutional neural network (CNN) for detecting the presence of vulnerabilities in complete code.

\subsubsection*{\bf Evaluation Metrics}
We adopt the same metrics in Wu et al.~\cite{wu2022vulcnn} to
evaluate VulCNN, i.e., \textit{true positive rate} (TPR) (also
referred to as \textit{Recall}), \textit{true negative rate} (TNR),
and \textit{F-score}. Here, the positive label corresponds to the
presence of a vulnerability in the method under study, while the
negative label is given to a non-vulnerable method.

%Thus, \textit{true positives}, i.e., $TP$ can be defined as the number of vulnerable methods that have been identified correctly; \textit{false positives}, i.e., $FP$ is the number of non-vulnerable methods that have been identified as vulnerable; \textit{false negative}, i.e., $FN$ is the number of vulnerable methods that have been identified as non-vulnerable; and \textit{true negatives} are the number of non-vulnerable methods that have been identified correctly.

\subsubsection*{\bf Preliminary Experimental Results}
%In Table~\ref{tab:extr-method}, we report the performance of VulCNN
%from both experiment settings,
Table~\ref{tab:extr-method} shows VulCNN's performance in both
settings, i.e., by using PDG\textsuperscript{\#} and
PDG\textsuperscript{*}. We can observe that \tool predicts
PDG\textsuperscript{*} with an overall F-score of 91.13\%. This
further establishes the {\em generalizability of \tool}, more so,
because the Java dataset that \tool was trained on, and the VD dataset
comprising Java methods for which PDG\textsuperscript{*}s were derived
come from two {\em entirely distinct code corpora}. Moreover, VulCNN
achieves an F-score of 73.26\% using PDG\textsuperscript{*}, which is
a close approximate of the F-score of VulCNN using
PDG\textsuperscript{\#}, 74.01\%.

\begin{tcolorbox}
{The PDGs predicted by \tool approximates the accuracy of those
  generated by program analysis for vulnerability detection on
  complete code by \underline{98.98\%}}.
\end{tcolorbox}

%\input{tables/extrinsic}

%\footnote{True positive rate is also referred to as \textit{Recall}.}
%the downstream task of 

\begin{table}[t]
  \centering
  \small
%  \scriptsize
%  \tabcolsep 3pt
  \caption{Comparison of PDG\textsuperscript{\#} (generated by PA tool) and PDG* (predicted by \tool) for  method-level VD.}
%  \vspace{-0.06in}
%\toprule
\begin{tabular}{c|c|c|c}
\hline
\textbf{Methodology}                       & \textbf{TPR}       & \textbf{TNR} & \textbf{F-Score} \\ \hline
\textit{PDG\textsuperscript{\#} + VulCNN}  &  74.03             &  74.03       & 74.01                  \\
\textit{PDG\textsuperscript{*} + VulCNN}   &  73.27             &  73.27       & 73.26\\
\hline
\end{tabular}
\label{tab:extr-method}
\end{table}
